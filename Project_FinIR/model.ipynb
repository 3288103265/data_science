{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as  plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            label    open     high     low   close   volume        oi\ndate                                                                 \n2003-01-02    1.0  1337.0  1356.00  1335.0  1353.5      NaN       NaN\n2003-01-03    1.0  1359.0  1360.00  1345.0  1348.0      NaN       NaN\n2003-01-06    1.0  1348.0  1350.00  1340.0  1349.0      NaN       NaN\n2003-01-07    1.0  1341.0  1351.00  1341.0  1348.5      NaN       NaN\n2003-01-08    1.0  1347.0  1360.00  1346.0  1357.0      NaN       NaN\n...           ...     ...      ...     ...     ...      ...       ...\n2017-12-21    1.0  2122.0  2151.00  2113.5  2148.0  67302.0  113508.0\n2017-12-22    1.0  2148.0  2204.00  2130.0  2192.0  58764.0    6750.0\n2017-12-27    0.0  2190.0  2254.00  2170.0  2252.0  71699.0    6238.0\n2017-12-28    0.0  2247.0  2284.75  2228.0  2284.0  72495.0    9737.0\n2017-12-29    0.0  2270.5  2290.50  2250.5  2268.0  57586.0    4314.0\n\n[3790 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>oi</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2003-01-02</th>\n      <td>1.0</td>\n      <td>1337.0</td>\n      <td>1356.00</td>\n      <td>1335.0</td>\n      <td>1353.5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2003-01-03</th>\n      <td>1.0</td>\n      <td>1359.0</td>\n      <td>1360.00</td>\n      <td>1345.0</td>\n      <td>1348.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2003-01-06</th>\n      <td>1.0</td>\n      <td>1348.0</td>\n      <td>1350.00</td>\n      <td>1340.0</td>\n      <td>1349.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2003-01-07</th>\n      <td>1.0</td>\n      <td>1341.0</td>\n      <td>1351.00</td>\n      <td>1341.0</td>\n      <td>1348.5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2003-01-08</th>\n      <td>1.0</td>\n      <td>1347.0</td>\n      <td>1360.00</td>\n      <td>1346.0</td>\n      <td>1357.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2017-12-21</th>\n      <td>1.0</td>\n      <td>2122.0</td>\n      <td>2151.00</td>\n      <td>2113.5</td>\n      <td>2148.0</td>\n      <td>67302.0</td>\n      <td>113508.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-22</th>\n      <td>1.0</td>\n      <td>2148.0</td>\n      <td>2204.00</td>\n      <td>2130.0</td>\n      <td>2192.0</td>\n      <td>58764.0</td>\n      <td>6750.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-27</th>\n      <td>0.0</td>\n      <td>2190.0</td>\n      <td>2254.00</td>\n      <td>2170.0</td>\n      <td>2252.0</td>\n      <td>71699.0</td>\n      <td>6238.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-28</th>\n      <td>0.0</td>\n      <td>2247.0</td>\n      <td>2284.75</td>\n      <td>2228.0</td>\n      <td>2284.0</td>\n      <td>72495.0</td>\n      <td>9737.0</td>\n    </tr>\n    <tr>\n      <th>2017-12-29</th>\n      <td>0.0</td>\n      <td>2270.5</td>\n      <td>2290.50</td>\n      <td>2250.5</td>\n      <td>2268.0</td>\n      <td>57586.0</td>\n      <td>4314.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3790 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# 处理铝20天\n",
    "label_20 = pd.read_csv('/home/wph/course/data_science/Project_FinIR/Train_data/Label_LMEAluminium_train_20d.csv', names=['index','date','label'],header=0,index_col='date').drop('index',axis=1)\n",
    "lv = pd.read_csv('/home/wph/course/data_science/Project_FinIR/Train_data/LMEAluminium3M_train.csv',names=['index','date','open','high','low','close','volume'], header=0,index_col='date').drop('index', axis=1)\n",
    "lv_oi = pd.read_csv('/home/wph/course/data_science/Project_FinIR/Train_data/LMEAluminium_OI_train.csv', names=['index','date','oi'],header=0,index_col='date').drop('index',axis=1)\n",
    "\n",
    "feature_list = [lv, lv_oi]\n",
    "\n",
    "indices_path = glob.glob(r'Train_data/Indices*')\n",
    "indices_path.sort()\n",
    "indices_name = ['dxy','nky', 'shsz','spx','sx5e','ukx','vix']\n",
    "for i, path in enumerate(indices_path):\n",
    "    feature_list.append(pd.read_csv(path, names=['index','date',indices_name[i]], header=0, index_col='date').drop('index',axis=1))\n",
    "\n",
    "train = label_20.join(feature_list, how='left')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Missing ratio:\nlabel:0.0\nopen:0.0\nhigh:0.0\nlow:0.0\nclose:0.0\nvolume:0.06411609498680738\noi:0.10158311345646438\n"
    }
   ],
   "source": [
    "# missing value\n",
    "cols = train.select_dtypes('number')\n",
    "print('Missing ratio:')\n",
    "for col in cols:\n",
    "    print('{}:{}'.format(col, train[col].isna().sum()/len(train)))\n",
    "    med = train[col].median()\n",
    "    train[col].fillna(med,inplace=True)\n",
    "\n",
    "target = train['label'].values\n",
    "data = train.drop('label',axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "# from sklearn.preprocessing import scale\n",
    "\n",
    "# data = scale(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将10个连续的数据拼接\n",
    "# concat_data = data\n",
    "# window_size=10\n",
    "# for i in range(window_size):\n",
    "#     data_shift = data.shift(i+1, fill_value=0)\n",
    "#     concat_data = pd.concat([concat_data, data_shift], axis=1)\n",
    "# concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = int(len(data)*0.8)\n",
    "X_valid = data[train_size:]\n",
    "Y_valid = target[train_size:]\n",
    "X_train = data[:train_size]\n",
    "Y_train = target[:train_size]\n",
    "# 打乱顺序\n",
    "X_train = np.random.shuffle(X_train)\n",
    "Y_train = np.random.shuffle(Y_train)\n",
    "\n",
    "\n",
    "D_train = lgb.Dataset(X_train, Y_train)\n",
    "D_valid = lgb.Dataset(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "params = dict(boosting_type='gbdt',\n",
    "            objective='binary',\n",
    "            # max_depth=10,\n",
    "            # num_leaves=10**2-1,\n",
    "            learning_rate=0.001,\n",
    "            min_child_weight=1,\n",
    "            # colsample_bytree=0.8,\n",
    "            # colsample_bynode=0.8,\n",
    "            reg_alpha=0,\n",
    "            reg_lambda=0.1,\n",
    "            n_jobs=-1,\n",
    "            random_state=2020,\n",
    "            device='gpu',\n",
    "            gpu_platform_id=0,\n",
    "            gpu_device_id=0,\n",
    "            #   first_metric_only=True,\n",
    "            metric=['binary_error']\n",
    "            )\n",
    "eval_results = dict()\n",
    "num_round = 5000\n",
    "valid_sets = [D_train, D_valid]\n",
    "valid_names = ['train', 'valid']\n",
    "lgb_reg = lgb.train(params,\n",
    "                    D_train,\n",
    "                    num_boost_round=num_round,\n",
    "                    valid_sets=valid_sets,\n",
    "                    valid_names=valid_names,\n",
    "                    early_stopping_rounds=50,\n",
    "                    evals_result=eval_results,\n",
    "                    verbose_eval=20\n",
    "                    # early_stopping_rounds=50\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3790, 32, 13)\n(3790, 1)\n"
    }
   ],
   "source": [
    "# 获取数据，按照给定的窗口，每个样本由当前七天的样本构成。\n",
    "window = 32\n",
    "X_train, Y_train=[],[]\n",
    "for i in range(window):\n",
    "    padding = np.zeros((window-1-i, data.shape[1]),dtype=float)\n",
    "    tmp = data[:i+1]\n",
    "    tmp = np.vstack((padding, tmp))\n",
    "    assert tmp.shape == (window, data.shape[1])\n",
    "    X_train.append(tmp)\n",
    "    Y_train.append(target[i])\n",
    "for i in range(window, len(train)):\n",
    "    X_train.append(data[i-window:i])\n",
    "    Y_train.append(target[i])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train).astype(float)\n",
    "Y_train = np.expand_dims(Y_train,-1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".5544 - val_loss: 5.0873 - val_accuracy: 0.5409 - lr: 2.8243e-04\nEpoch 189/300\n46/48 [===========================>..] - ETA: 0s - loss: 2.6922 - accuracy: 0.5435\nEpoch 00189: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n48/48 [==============================] - 0s 10ms/step - loss: 2.6895 - accuracy: 0.5432 - val_loss: 8.0578 - val_accuracy: 0.4538 - lr: 2.8243e-04\nEpoch 190/300\n48/48 [==============================] - 1s 11ms/step - loss: 4.5471 - accuracy: 0.5310 - val_loss: 6.9846 - val_accuracy: 0.5369 - lr: 2.5419e-04\nEpoch 191/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.5120 - accuracy: 0.5587 - val_loss: 3.3578 - val_accuracy: 0.5264 - lr: 2.5419e-04\nEpoch 192/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.1894 - accuracy: 0.5534 - val_loss: 8.9806 - val_accuracy: 0.5422 - lr: 2.5419e-04\nEpoch 193/300\n48/48 [==============================] - 0s 10ms/step - loss: 3.3770 - accuracy: 0.5402 - val_loss: 3.3170 - val_accuracy: 0.5198 - lr: 2.5419e-04\nEpoch 194/300\n48/48 [==============================] - 1s 11ms/step - loss: 2.4842 - accuracy: 0.5515 - val_loss: 4.0876 - val_accuracy: 0.4525 - lr: 2.5419e-04\nEpoch 195/300\n48/48 [==============================] - 0s 9ms/step - loss: 3.5000 - accuracy: 0.5419 - val_loss: 6.8158 - val_accuracy: 0.5409 - lr: 2.5419e-04\nEpoch 196/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.4382 - accuracy: 0.5462 - val_loss: 2.6077 - val_accuracy: 0.4565 - lr: 2.5419e-04\nEpoch 197/300\n48/48 [==============================] - 1s 11ms/step - loss: 2.9831 - accuracy: 0.5561 - val_loss: 4.6693 - val_accuracy: 0.5343 - lr: 2.5419e-04\nEpoch 198/300\n48/48 [==============================] - 0s 10ms/step - loss: 3.3751 - accuracy: 0.5359 - val_loss: 5.6955 - val_accuracy: 0.5422 - lr: 2.5419e-04\nEpoch 199/300\n47/48 [============================>.] - ETA: 0s - loss: 3.0101 - accuracy: 0.5436\nEpoch 00199: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n48/48 [==============================] - 1s 11ms/step - loss: 3.0028 - accuracy: 0.5432 - val_loss: 4.7745 - val_accuracy: 0.4459 - lr: 2.5419e-04\nEpoch 200/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.9158 - accuracy: 0.5435 - val_loss: 5.2990 - val_accuracy: 0.4446 - lr: 2.2877e-04\nEpoch 201/300\n48/48 [==============================] - 0s 8ms/step - loss: 2.5766 - accuracy: 0.5547 - val_loss: 8.8136 - val_accuracy: 0.5409 - lr: 2.2877e-04\nEpoch 202/300\n48/48 [==============================] - 0s 8ms/step - loss: 3.0557 - accuracy: 0.5491 - val_loss: 2.3228 - val_accuracy: 0.4828 - lr: 2.2877e-04\nEpoch 203/300\n48/48 [==============================] - 1s 11ms/step - loss: 3.1558 - accuracy: 0.5521 - val_loss: 2.2709 - val_accuracy: 0.4565 - lr: 2.2877e-04\nEpoch 204/300\n48/48 [==============================] - 0s 8ms/step - loss: 2.5926 - accuracy: 0.5567 - val_loss: 2.6027 - val_accuracy: 0.5145 - lr: 2.2877e-04\nEpoch 205/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.6656 - accuracy: 0.5359 - val_loss: 2.9515 - val_accuracy: 0.5251 - lr: 2.2877e-04\nEpoch 206/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.6428 - accuracy: 0.5554 - val_loss: 4.3655 - val_accuracy: 0.4578 - lr: 2.2877e-04\nEpoch 207/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.1428 - accuracy: 0.5607 - val_loss: 6.1601 - val_accuracy: 0.5422 - lr: 2.2877e-04\nEpoch 208/300\n48/48 [==============================] - 0s 8ms/step - loss: 2.4598 - accuracy: 0.5604 - val_loss: 7.4217 - val_accuracy: 0.5409 - lr: 2.2877e-04\nEpoch 209/300\n47/48 [============================>.] - ETA: 0s - loss: 2.6829 - accuracy: 0.5479\nEpoch 00209: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n48/48 [==============================] - 0s 10ms/step - loss: 2.6738 - accuracy: 0.5482 - val_loss: 4.8862 - val_accuracy: 0.5383 - lr: 2.2877e-04\nEpoch 210/300\n48/48 [==============================] - 0s 8ms/step - loss: 2.2131 - accuracy: 0.5574 - val_loss: 2.4342 - val_accuracy: 0.4908 - lr: 2.0589e-04\nEpoch 211/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.7515 - accuracy: 0.5458 - val_loss: 2.4888 - val_accuracy: 0.5145 - lr: 2.0589e-04\nEpoch 212/300\n48/48 [==============================] - 0s 8ms/step - loss: 2.8399 - accuracy: 0.5518 - val_loss: 5.0720 - val_accuracy: 0.5409 - lr: 2.0589e-04\nEpoch 213/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.4639 - accuracy: 0.5627 - val_loss: 2.8715 - val_accuracy: 0.5092 - lr: 2.0589e-04\nEpoch 214/300\n48/48 [==============================] - 0s 8ms/step - loss: 2.4206 - accuracy: 0.5594 - val_loss: 2.4651 - val_accuracy: 0.5092 - lr: 2.0589e-04\nEpoch 215/300\n48/48 [==============================] - 1s 11ms/step - loss: 1.9917 - accuracy: 0.5571 - val_loss: 2.7927 - val_accuracy: 0.4393 - lr: 2.0589e-04\nEpoch 216/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.9996 - accuracy: 0.5726 - val_loss: 2.3731 - val_accuracy: 0.4881 - lr: 2.0589e-04\nEpoch 217/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.6818 - accuracy: 0.5547 - val_loss: 2.5611 - val_accuracy: 0.4617 - lr: 2.0589e-04\nEpoch 218/300\n48/48 [==============================] - 1s 13ms/step - loss: 2.1455 - accuracy: 0.5716 - val_loss: 2.6565 - val_accuracy: 0.4815 - lr: 2.0589e-04\nEpoch 219/300\n46/48 [===========================>..] - ETA: 0s - loss: 2.3110 - accuracy: 0.5482\nEpoch 00219: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n48/48 [==============================] - 0s 10ms/step - loss: 2.2983 - accuracy: 0.5478 - val_loss: 2.2587 - val_accuracy: 0.4894 - lr: 2.0589e-04\nEpoch 220/300\n48/48 [==============================] - 1s 11ms/step - loss: 2.1922 - accuracy: 0.5557 - val_loss: 2.5403 - val_accuracy: 0.5158 - lr: 1.8530e-04\nEpoch 221/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.7890 - accuracy: 0.5505 - val_loss: 7.3322 - val_accuracy: 0.5422 - lr: 1.8530e-04\nEpoch 222/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.4100 - accuracy: 0.5653 - val_loss: 4.6089 - val_accuracy: 0.5422 - lr: 1.8530e-04\nEpoch 223/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.0881 - accuracy: 0.5729 - val_loss: 2.6396 - val_accuracy: 0.4723 - lr: 1.8530e-04\nEpoch 224/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.2611 - accuracy: 0.5554 - val_loss: 3.1455 - val_accuracy: 0.5211 - lr: 1.8530e-04\nEpoch 225/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.8096 - accuracy: 0.5607 - val_loss: 2.4877 - val_accuracy: 0.4472 - lr: 1.8530e-04\nEpoch 226/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.0767 - accuracy: 0.5528 - val_loss: 2.3659 - val_accuracy: 0.5106 - lr: 1.8530e-04\nEpoch 227/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.4314 - accuracy: 0.5485 - val_loss: 2.8228 - val_accuracy: 0.5158 - lr: 1.8530e-04\nEpoch 228/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.2807 - accuracy: 0.5538 - val_loss: 6.8887 - val_accuracy: 0.5422 - lr: 1.8530e-04\nEpoch 229/300\n45/48 [===========================>..] - ETA: 0s - loss: 2.0527 - accuracy: 0.5521\nEpoch 00229: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n48/48 [==============================] - 0s 10ms/step - loss: 2.0636 - accuracy: 0.5524 - val_loss: 3.4850 - val_accuracy: 0.4485 - lr: 1.8530e-04\nEpoch 230/300\n48/48 [==============================] - 0s 7ms/step - loss: 2.4778 - accuracy: 0.5551 - val_loss: 4.1397 - val_accuracy: 0.5383 - lr: 1.6677e-04\nEpoch 231/300\n48/48 [==============================] - 1s 12ms/step - loss: 1.9314 - accuracy: 0.5604 - val_loss: 2.3444 - val_accuracy: 0.4842 - lr: 1.6677e-04\nEpoch 232/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.1197 - accuracy: 0.5488 - val_loss: 2.3860 - val_accuracy: 0.5092 - lr: 1.6677e-04\nEpoch 233/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.2279 - accuracy: 0.5683 - val_loss: 2.5823 - val_accuracy: 0.4380 - lr: 1.6677e-04\nEpoch 234/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.9269 - accuracy: 0.5458 - val_loss: 2.1228 - val_accuracy: 0.4697 - lr: 1.6677e-04\nEpoch 235/300\n48/48 [==============================] - 1s 11ms/step - loss: 1.9395 - accuracy: 0.5627 - val_loss: 4.8983 - val_accuracy: 0.5409 - lr: 1.6677e-04\nEpoch 236/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.1646 - accuracy: 0.5449 - val_loss: 2.5669 - val_accuracy: 0.4406 - lr: 1.6677e-04\nEpoch 237/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.2662 - accuracy: 0.5646 - val_loss: 3.3414 - val_accuracy: 0.4446 - lr: 1.6677e-04\nEpoch 238/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.8553 - accuracy: 0.5419 - val_loss: 2.4469 - val_accuracy: 0.4921 - lr: 1.6677e-04\nEpoch 239/300\n43/48 [=========================>....] - ETA: 0s - loss: 2.1925 - accuracy: 0.5694\nEpoch 00239: ReduceLROnPlateau reducing learning rate to 0.00015009464841568844.\n48/48 [==============================] - 0s 10ms/step - loss: 2.1641 - accuracy: 0.5663 - val_loss: 4.3486 - val_accuracy: 0.5383 - lr: 1.6677e-04\nEpoch 240/300\n48/48 [==============================] - 0s 8ms/step - loss: 2.2075 - accuracy: 0.5455 - val_loss: 4.5177 - val_accuracy: 0.5396 - lr: 1.5009e-04\nEpoch 241/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.3983 - accuracy: 0.5392 - val_loss: 4.3552 - val_accuracy: 0.4538 - lr: 1.5009e-04\nEpoch 242/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.3425 - accuracy: 0.5416 - val_loss: 2.2986 - val_accuracy: 0.5119 - lr: 1.5009e-04\nEpoch 243/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.0982 - accuracy: 0.5617 - val_loss: 2.2174 - val_accuracy: 0.4908 - lr: 1.5009e-04\nEpoch 244/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.9771 - accuracy: 0.5571 - val_loss: 3.7628 - val_accuracy: 0.4538 - lr: 1.5009e-04\nEpoch 245/300\n48/48 [==============================] - 1s 12ms/step - loss: 2.2946 - accuracy: 0.5554 - val_loss: 2.1507 - val_accuracy: 0.4670 - lr: 1.5009e-04\nEpoch 246/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.5465 - accuracy: 0.5495 - val_loss: 3.3056 - val_accuracy: 0.4340 - lr: 1.5009e-04\nEpoch 247/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.7555 - accuracy: 0.5590 - val_loss: 3.2062 - val_accuracy: 0.5172 - lr: 1.5009e-04\nEpoch 248/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.7160 - accuracy: 0.5676 - val_loss: 2.2352 - val_accuracy: 0.4987 - lr: 1.5009e-04\nEpoch 249/300\n45/48 [===========================>..] - ETA: 0s - loss: 2.2611 - accuracy: 0.5521\nEpoch 00249: ReduceLROnPlateau reducing learning rate to 0.0001350851875031367.\n48/48 [==============================] - 1s 11ms/step - loss: 2.2487 - accuracy: 0.5547 - val_loss: 4.5086 - val_accuracy: 0.4538 - lr: 1.5009e-04\nEpoch 250/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.9738 - accuracy: 0.5488 - val_loss: 4.4781 - val_accuracy: 0.4551 - lr: 1.3509e-04\nEpoch 251/300\n48/48 [==============================] - 0s 8ms/step - loss: 2.0869 - accuracy: 0.5544 - val_loss: 2.9447 - val_accuracy: 0.4301 - lr: 1.3509e-04\nEpoch 252/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.9964 - accuracy: 0.5521 - val_loss: 4.0942 - val_accuracy: 0.4472 - lr: 1.3509e-04\nEpoch 253/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.0253 - accuracy: 0.5571 - val_loss: 4.9562 - val_accuracy: 0.4551 - lr: 1.3509e-04\nEpoch 254/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.3151 - accuracy: 0.5439 - val_loss: 2.4443 - val_accuracy: 0.4644 - lr: 1.3509e-04\nEpoch 255/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.8669 - accuracy: 0.5633 - val_loss: 2.6589 - val_accuracy: 0.4340 - lr: 1.3509e-04\nEpoch 256/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.8131 - accuracy: 0.5587 - val_loss: 3.8144 - val_accuracy: 0.5343 - lr: 1.3509e-04\nEpoch 257/300\n48/48 [==============================] - 0s 8ms/step - loss: 2.0103 - accuracy: 0.5633 - val_loss: 2.8228 - val_accuracy: 0.4736 - lr: 1.3509e-04\nEpoch 258/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.9104 - accuracy: 0.5498 - val_loss: 2.6299 - val_accuracy: 0.5026 - lr: 1.3509e-04\nEpoch 259/300\n42/48 [=========================>....] - ETA: 0s - loss: 1.8500 - accuracy: 0.5528\nEpoch 00259: ReduceLROnPlateau reducing learning rate to 0.00012157666351413355.\n48/48 [==============================] - 0s 9ms/step - loss: 1.8494 - accuracy: 0.5495 - val_loss: 3.2288 - val_accuracy: 0.4314 - lr: 1.3509e-04\nEpoch 260/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.2382 - accuracy: 0.5425 - val_loss: 3.5573 - val_accuracy: 0.4367 - lr: 1.2158e-04\nEpoch 261/300\n48/48 [==============================] - 0s 8ms/step - loss: 1.7462 - accuracy: 0.5670 - val_loss: 3.3868 - val_accuracy: 0.4288 - lr: 1.2158e-04\nEpoch 262/300\n48/48 [==============================] - 1s 12ms/step - loss: 2.5571 - accuracy: 0.5564 - val_loss: 2.9314 - val_accuracy: 0.4222 - lr: 1.2158e-04\nEpoch 263/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.3538 - accuracy: 0.5554 - val_loss: 2.4504 - val_accuracy: 0.4868 - lr: 1.2158e-04\nEpoch 264/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.2789 - accuracy: 0.5597 - val_loss: 3.2889 - val_accuracy: 0.4248 - lr: 1.2158e-04\nEpoch 265/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.8604 - accuracy: 0.5567 - val_loss: 3.7375 - val_accuracy: 0.5317 - lr: 1.2158e-04\nEpoch 266/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.9620 - accuracy: 0.5518 - val_loss: 3.7056 - val_accuracy: 0.4380 - lr: 1.2158e-04\nEpoch 267/300\n48/48 [==============================] - 0s 10ms/step - loss: 2.2133 - accuracy: 0.5554 - val_loss: 3.5929 - val_accuracy: 0.5277 - lr: 1.2158e-04\nEpoch 268/300\n48/48 [==============================] - 0s 8ms/step - loss: 1.8151 - accuracy: 0.5623 - val_loss: 2.5758 - val_accuracy: 0.5026 - lr: 1.2158e-04\nEpoch 269/300\n43/48 [=========================>....] - ETA: 0s - loss: 1.8135 - accuracy: 0.5618\nEpoch 00269: ReduceLROnPlateau reducing learning rate to 0.00010941899454337544.\n48/48 [==============================] - 0s 10ms/step - loss: 1.8047 - accuracy: 0.5610 - val_loss: 3.3129 - val_accuracy: 0.5158 - lr: 1.2158e-04\nEpoch 270/300\n48/48 [==============================] - 0s 8ms/step - loss: 1.9301 - accuracy: 0.5571 - val_loss: 3.4196 - val_accuracy: 0.5211 - lr: 1.0942e-04\nEpoch 271/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.9908 - accuracy: 0.5580 - val_loss: 2.6120 - val_accuracy: 0.4393 - lr: 1.0942e-04\nEpoch 272/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.8264 - accuracy: 0.5650 - val_loss: 2.7361 - val_accuracy: 0.4406 - lr: 1.0942e-04\nEpoch 273/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.9530 - accuracy: 0.5501 - val_loss: 3.1691 - val_accuracy: 0.5198 - lr: 1.0942e-04\nEpoch 274/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.8702 - accuracy: 0.5597 - val_loss: 6.0400 - val_accuracy: 0.5409 - lr: 1.0942e-04\nEpoch 275/300\n48/48 [==============================] - 0s 8ms/step - loss: 1.7852 - accuracy: 0.5554 - val_loss: 2.4964 - val_accuracy: 0.4749 - lr: 1.0942e-04\nEpoch 276/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.7620 - accuracy: 0.5531 - val_loss: 4.4471 - val_accuracy: 0.4499 - lr: 1.0942e-04\nEpoch 277/300\n48/48 [==============================] - 1s 11ms/step - loss: 2.0161 - accuracy: 0.5547 - val_loss: 3.2495 - val_accuracy: 0.5172 - lr: 1.0942e-04\nEpoch 278/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.8361 - accuracy: 0.5587 - val_loss: 2.9604 - val_accuracy: 0.4736 - lr: 1.0942e-04\nEpoch 279/300\n44/48 [==========================>...] - ETA: 0s - loss: 1.8262 - accuracy: 0.5529\nEpoch 00279: ReduceLROnPlateau reducing learning rate to 9.847709443420172e-05.\n48/48 [==============================] - 0s 10ms/step - loss: 1.8290 - accuracy: 0.5524 - val_loss: 2.4890 - val_accuracy: 0.4657 - lr: 1.0942e-04\nEpoch 280/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.8345 - accuracy: 0.5518 - val_loss: 3.7873 - val_accuracy: 0.5317 - lr: 9.8477e-05\nEpoch 281/300\n48/48 [==============================] - 1s 12ms/step - loss: 2.0291 - accuracy: 0.5495 - val_loss: 3.0327 - val_accuracy: 0.5211 - lr: 9.8477e-05\nEpoch 282/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.7261 - accuracy: 0.5620 - val_loss: 2.9113 - val_accuracy: 0.5145 - lr: 9.8477e-05\nEpoch 283/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.7858 - accuracy: 0.5653 - val_loss: 3.8580 - val_accuracy: 0.5343 - lr: 9.8477e-05\nEpoch 284/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.5998 - accuracy: 0.5679 - val_loss: 4.3826 - val_accuracy: 0.5396 - lr: 9.8477e-05\nEpoch 285/300\n48/48 [==============================] - 0s 8ms/step - loss: 1.8774 - accuracy: 0.5587 - val_loss: 5.5901 - val_accuracy: 0.5409 - lr: 9.8477e-05\nEpoch 286/300\n48/48 [==============================] - 1s 10ms/step - loss: 2.0540 - accuracy: 0.5627 - val_loss: 4.2364 - val_accuracy: 0.4499 - lr: 9.8477e-05\nEpoch 287/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.9969 - accuracy: 0.5511 - val_loss: 3.1577 - val_accuracy: 0.5251 - lr: 9.8477e-05\nEpoch 288/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.3515 - accuracy: 0.5683 - val_loss: 2.9768 - val_accuracy: 0.5119 - lr: 9.8477e-05\nEpoch 289/300\n45/48 [===========================>..] - ETA: 0s - loss: 1.8980 - accuracy: 0.5465\nEpoch 00289: ReduceLROnPlateau reducing learning rate to 8.862938630045391e-05.\n48/48 [==============================] - 0s 9ms/step - loss: 1.8973 - accuracy: 0.5468 - val_loss: 6.0068 - val_accuracy: 0.5409 - lr: 9.8477e-05\nEpoch 290/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.7963 - accuracy: 0.5508 - val_loss: 2.7547 - val_accuracy: 0.5026 - lr: 8.8629e-05\nEpoch 291/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.9647 - accuracy: 0.5587 - val_loss: 3.3347 - val_accuracy: 0.5158 - lr: 8.8629e-05\nEpoch 292/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.0005 - accuracy: 0.5475 - val_loss: 2.8797 - val_accuracy: 0.5158 - lr: 8.8629e-05\nEpoch 293/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.7767 - accuracy: 0.5538 - val_loss: 3.8673 - val_accuracy: 0.5330 - lr: 8.8629e-05\nEpoch 294/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.7310 - accuracy: 0.5547 - val_loss: 3.4996 - val_accuracy: 0.4340 - lr: 8.8629e-05\nEpoch 295/300\n48/48 [==============================] - 1s 11ms/step - loss: 1.7768 - accuracy: 0.5557 - val_loss: 2.4680 - val_accuracy: 0.4749 - lr: 8.8629e-05\nEpoch 296/300\n48/48 [==============================] - 0s 9ms/step - loss: 1.7675 - accuracy: 0.5590 - val_loss: 2.5236 - val_accuracy: 0.4789 - lr: 8.8629e-05\nEpoch 297/300\n48/48 [==============================] - 0s 10ms/step - loss: 1.8307 - accuracy: 0.5524 - val_loss: 3.4787 - val_accuracy: 0.5264 - lr: 8.8629e-05\nEpoch 298/300\n48/48 [==============================] - 1s 12ms/step - loss: 2.0518 - accuracy: 0.5541 - val_loss: 2.4754 - val_accuracy: 0.4776 - lr: 8.8629e-05\nEpoch 299/300\n42/48 [=========================>....] - ETA: 0s - loss: 1.6880 - accuracy: 0.5472\nEpoch 00299: ReduceLROnPlateau reducing learning rate to 7.976644701557234e-05.\n48/48 [==============================] - 0s 9ms/step - loss: 1.6890 - accuracy: 0.5458 - val_loss: 4.3301 - val_accuracy: 0.5356 - lr: 8.8629e-05\nEpoch 300/300\n48/48 [==============================] - 0s 9ms/step - loss: 2.1492 - accuracy: 0.5551 - val_loss: 2.6023 - val_accuracy: 0.4354 - lr: 7.9766e-05\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fbed01584a8>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# 使用tcn\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "\n",
    "batch_size , timesteps, input_dim = None, window, X_train.shape[2]\n",
    "\n",
    "i = Input(batch_shape=(batch_size, timesteps, input_dim))\n",
    "\n",
    "o = TCN(dilations=[1,2,4,8,16], return_sequences=False, nb_filters=16, dropout_rate=0.1)(i)\n",
    "o = Dense(1, activation='sigmoid')(o)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.7, patience=10, verbose=10, mode='auto',\n",
    "    min_delta=0.0001, cooldown=0, min_lr=0.00005\n",
    ")\n",
    "\n",
    "optimizer=keras.optimizers.RMSprop(learning_rate=0.003)\t\n",
    "\n",
    "m = Model(inputs=[i], outputs=[o])\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "tcn_full_summary(m, expand_residual_blocks=False)\n",
    "\n",
    "m.fit(X_train,Y_train, epochs=300, batch_size=64, validation_split=0.2, shuffle=True, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4\n(1000, 20, 1)\n(1000, 1)\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 20, 1)]           0         \n_________________________________________________________________\nresidual_block_0 (ResidualBl [(None, 20, 64), (None, 2 8576      \n_________________________________________________________________\nresidual_block_1 (ResidualBl [(None, 20, 64), (None, 2 16512     \n_________________________________________________________________\nresidual_block_2 (ResidualBl [(None, 20, 64), (None, 2 16512     \n_________________________________________________________________\nresidual_block_3 (ResidualBl [(None, 20, 64), (None, 2 16512     \n_________________________________________________________________\nresidual_block_4 (ResidualBl [(None, 20, 64), (None, 2 16512     \n_________________________________________________________________\nresidual_block_5 (ResidualBl [(None, 20, 64), (None, 2 16512     \n_________________________________________________________________\nlambda (Lambda)              (None, 64)                0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 65        \n=================================================================\nTotal params: 91,201\nTrainable params: 91,201\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n25/25 [==============================] - 1s 26ms/step - loss: 0.1815 - val_loss: 0.0011\nEpoch 2/10\n25/25 [==============================] - 0s 9ms/step - loss: 9.2981e-04 - val_loss: 3.0370e-04\nEpoch 3/10\n25/25 [==============================] - 0s 11ms/step - loss: 6.9652e-05 - val_loss: 3.8283e-05\nEpoch 4/10\n25/25 [==============================] - 0s 10ms/step - loss: 8.7461e-06 - val_loss: 2.7041e-06\nEpoch 5/10\n25/25 [==============================] - 0s 9ms/step - loss: 6.6175e-07 - val_loss: 6.4476e-08\nEpoch 6/10\n25/25 [==============================] - 0s 8ms/step - loss: 4.0151e-08 - val_loss: 6.0398e-11\nEpoch 7/10\n25/25 [==============================] - 0s 11ms/step - loss: 1.4571e-09 - val_loss: 6.2110e-11\nEpoch 8/10\n25/25 [==============================] - 0s 9ms/step - loss: 9.1821e-11 - val_loss: 1.3022e-11\nEpoch 9/10\n25/25 [==============================] - 0s 8ms/step - loss: 7.3551e-12 - val_loss: 1.3715e-13\nEpoch 10/10\n25/25 [==============================] - 0s 8ms/step - loss: 5.0119e-13 - val_loss: 7.0583e-15\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fdcec0d4f60>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "import tensorflow as tf\n",
    "from tcn import TCN, tcn_full_summary\n",
    "\n",
    "print(len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "batch_size, timesteps, input_dim = None, 20, 1\n",
    "\n",
    "\n",
    "def get_x_y(size=1000):\n",
    "    import numpy as np\n",
    "    pos_indices = np.random.choice(size, size=int(size // 2), replace=False)\n",
    "    x_train = np.zeros(shape=(size, timesteps, 1))\n",
    "    y_train = np.zeros(shape=(size, 1))\n",
    "    x_train[pos_indices, 0] = 1.0\n",
    "    y_train[pos_indices, 0] = 1.0\n",
    "    return x_train, y_train\n",
    "\n",
    "x, y = get_x_y()\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "i = Input(batch_shape=(batch_size, timesteps, input_dim))\n",
    "\n",
    "o = TCN(return_sequences=False)(i)  # The TCN layers are here.\n",
    "o = Dense(1)(o)\n",
    "\n",
    "m = Model(inputs=[i], outputs=[o])\n",
    "m.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "tcn_full_summary(m, expand_residual_blocks=False)\n",
    "\n",
    "m.fit(x, y, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     a   b\n0    0   1\n1    1   2\n2    2   3\n3    3   4\n4    4   5\n5    5   6\n6    6   7\n7    7   8\n8    8   9\n9    9  10\n10  10  11\n11  11  12",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# 测试rolling\n",
    "import pandas as pd \n",
    "df = pd.DataFrame({'a':list(range(12)), 'b':list(range(1,13))})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NotImplementedError",
     "evalue": "See issue #11704 https://github.com/pandas-dev/pandas/issues/11704",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3e1a7b5c3c0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://github.com/pandas-dev/pandas/issues/11704\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"See issue #11704 {url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prep_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: See issue #11704 https://github.com/pandas-dev/pandas/issues/11704"
     ]
    }
   ],
   "source": [
    "df.rolling(3,min_periods=1)\n",
    "for a,b in df.rolling(3):\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitbasecondaf475587bc1de4035baa9301b6d40e2a0",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}